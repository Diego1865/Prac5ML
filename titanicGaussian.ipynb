# ============================================================================
# ANÁLISIS Y PREDICCIÓN DE SUPERVIVENCIA DEL TITANIC
# Modelo: Gaussian Naive Bayes (GaussianNB)
# ============================================================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from matplotlib.colors import ListedColormap

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report
)
from sklearn.decomposition import PCA

# -----------------------------
# Parámetros
# -----------------------------
TEST_SIZE = 0.25
RANDOM_STATE = 42

# -----------------------------
# 1) Cargar y Preparar Dataset
# -----------------------------
# Cargar dataset de Titanic desde seaborn
df = sns.load_dataset('titanic')

print("Dataset: Titanic - Predicción de Supervivencia")
print(f"Shape del Dataset: {df.shape}")
print(f"\nPrimeras filas:\n{df.head()}")
print(f"\nInformación del Dataset:")
print(df.info())
print(f"\nColumnas disponibles: {df.columns.tolist()}")
print("-----------------------------")

# Separar características (X) y variable objetivo (y)
# Eliminamos columnas irrelevantes o con demasiados valores faltantes
columns_to_drop = ['deck', 'embark_town', 'alive', 'alone', 'adult_male', 'who', 'class']
X = df.drop(columns=columns_to_drop + ['survived'], errors='ignore')
y = df['survived']

target_names = ['No Sobrevivió', 'Sobrevivió']

# -----------------------------
# 1.A) Gráfica 'antes' (EDA: Supervivencia por Género)
# -----------------------------
plt.figure(figsize=(8, 6), dpi=120)
sns.countplot(data=df, x='survived', hue='sex', palette='Set2')
plt.title("Antes: Distribución de Supervivencia por Género")
plt.xlabel("Supervivencia (0=No, 1=Sí)")
plt.ylabel("Cantidad de Pasajeros")
plt.xticks([0, 1], target_names)
plt.legend(title='Género', labels=['Femenino', 'Masculino'])
plt.tight_layout()
plt.show()

# -----------------------------
# 2) Preprocesamiento Avanzado con ColumnTransformer
# -----------------------------
# Definir características numéricas y categóricas
numeric_features = ['age', 'fare', 'sibsp', 'parch']
categorical_features = ['pclass', 'sex', 'embarked']

# Pipeline para características numéricas
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

# Pipeline para características categóricas
categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

# Combinar ambos pipelines con ColumnTransformer
preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_features),
        ('cat', categorical_transformer, categorical_features)
    ])

# -----------------------------
# 3) Split y Entrenamiento con Pipeline Completo
# -----------------------------
# Dividir el dataset
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y
)

# Definir el clasificador GaussianNB
classifier = GaussianNB()

# Pipeline completo: Preprocesamiento + Clasificador
pipe_gnb = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', classifier)
])

# Entrenar el modelo
pipe_gnb.fit(X_train, y_train)

# Predicción
y_pred = pipe_gnb.predict(X_test)

# -----------------------------
# 3.A) Métricas
# -----------------------------
# Cálculo de las 4 métricas requeridas
acc = accuracy_score(y_test, y_pred)
prec_macro = precision_score(y_test, y_pred, average="macro", zero_division=0)
recall_macro = recall_score(y_test, y_pred, average="macro", zero_division=0)
f1_macro = f1_score(y_test, y_pred, average="macro", zero_division=0)

print("\n=== GaussianNB: Métricas (macro) ===")
print(f"Accuracy:  {acc:.4f}")
print(f"Precision: {prec_macro:.4f}")
print(f"Recall:    {recall_macro:.4f}")
print(f"F1-score:  {f1_macro:.4f}")

print("\n=== GaussianNB: Reporte por clase ===")
print(classification_report(y_test, y_pred, target_names=target_names, zero_division=0))

# -----------------------------
# 3.B) Matriz de confusión
# -----------------------------
cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(6, 5), dpi=120)
plt.imshow(cm, interpolation="nearest", cmap=plt.cm.viridis) 
plt.title("Matriz de confusión (GaussianNB)")
plt.xlabel("Predicción")
plt.ylabel("Real")
plt.xticks(np.arange(len(target_names)), target_names, rotation=45, ha="right")
plt.yticks(np.arange(len(target_names)), target_names)
# Añadir los conteos a la matriz
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        plt.text(j, i, cm[i, j], ha="center", va="center", fontsize=11, 
                 color="white" if cm[i, j] > cm.max()/2 else "black")
plt.tight_layout()
plt.show()

# -----------------------------
# 4) Gráfica de Regiones de Decisión (Usando PCA 2D)
# -----------------------------
# Aplicar el preprocesador a todos los datos para obtener características procesadas
X_processed = preprocessor.fit_transform(X)

# Aplicar PCA para reducir a 2 componentes
pca = PCA(n_components=2, random_state=RANDOM_STATE)
X_pca = pca.fit_transform(X_processed)

# Dividir los datos reducidos por PCA
X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(
    X_pca, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y
)

# Entrenar un nuevo clasificador 2D
gnb_2d = GaussianNB()
gnb_2d.fit(X_train_pca, y_train_pca)

# Crear la Malla (meshgrid) para mapear las regiones de decisión
x_min, x_max = X_pca[:, 0].min() - 0.5, X_pca[:, 0].max() + 0.5
y_min, y_max = X_pca[:, 1].min() - 0.5, X_pca[:, 1].max() + 0.5
xx, yy = np.meshgrid(
    np.linspace(x_min, x_max, 400),
    np.linspace(y_min, y_max, 400)
)
Z = gnb_2d.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)

# Graficar
cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA'])
plt.figure(figsize=(7, 6), dpi=120)

# Contorno de regiones de decisión
plt.contourf(xx, yy, Z, cmap=cmap_light, alpha=0.5)

# Puntos de datos de prueba con su clase REAL
for label in np.unique(y_test_pca):
    mask = (y_test_pca == label)
    pts = X_test_pca[mask]
    plt.scatter(pts[:, 0], pts[:, 1], label=target_names[label], s=30)

plt.title("GaussianNB: Regiones de Decisión (PCA 2D)")
plt.xlabel(f"Componente Principal 1 (Explicación: {pca.explained_variance_ratio_[0]:.2f})")
plt.ylabel(f"Componente Principal 2 (Explicación: {pca.explained_variance_ratio_[1]:.2f})")
plt.legend()
plt.tight_layout()
plt.show()
