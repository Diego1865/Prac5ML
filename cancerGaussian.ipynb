# ==========================================================
# === Breast Cancer + GaussianNB (Naive Bayes)  ===
# ==========================================================

import numpy as np
import matplotlib.pyplot as plt
from matplotlib.colors import ListedColormap 

from sklearn.datasets import load_breast_cancer 
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    confusion_matrix, classification_report
)
from sklearn.decomposition import PCA

# -----------------------------
# Parámetros
# -----------------------------
TEST_SIZE = 0.25
RANDOM_STATE = 42

# -----------------------------
# 1) Cargar y Preparar Dataset
# -----------------------------
cancer = load_breast_cancer(as_frame=True)
X = cancer.data.copy()
y = cancer.target.copy()
target_names = cancer.target_names # ['malignant', 'benign']

print("Dataset: Breast Cancer Wisconsin")
print(f"Shape X: {X.shape} | Clases: {list(target_names)}")
print("-----------------------------")

# -----------------------------
# 1.A) Gráfica 'antes' (datos crudos, reducidos con PCA 2D)
# -----------------------------
# El escalado es crucial para PCA
scaler_pca = StandardScaler()
X_scaled = scaler_pca.fit_transform(X)

pca = PCA(n_components=2, random_state=RANDOM_STATE)
X_pca = pca.fit_transform(X_scaled) 

plt.figure(figsize=(7, 6), dpi=120)
for label in np.unique(y):
    mask = (y == label)
    plt.scatter(X_pca[mask, 0], X_pca[mask, 1],
                label=target_names[label], s=30)
plt.title("Antes: Dataset Breast Cancer (PCA 2D)")
plt.xlabel(f"Componente Principal 1 (Explicación: {pca.explained_variance_ratio_[0]:.2f})")
plt.ylabel(f"Componente Principal 2 (Explicación: {pca.explained_variance_ratio_[1]:.2f})")
plt.legend()
plt.tight_layout()
plt.show()

# -----------------------------
# 2) Split y Entrenamiento con Pipeline (Scaler -> GaussianNB)
# -----------------------------
# Dividir el dataset original (30 características)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y
)

# Definir el clasificador GaussianNB
classifier = GaussianNB()

# Crear pipeline con StandardScaler y GaussianNB
# Nota: Aunque GaussianNB no requiere estrictamente escalado, lo incluimos
# para mantener consistencia con el pipeline del ejemplo de DecisionTree
pipe_gnb = Pipeline([
    ("scaler", StandardScaler()),
    ("gnb", classifier)
])
pipe_gnb.fit(X_train, y_train)

# Predicción
y_pred = pipe_gnb.predict(X_test)

# -----------------------------
# 2.A) Métricas
# -----------------------------
# Cálculo de las 4 métricas requeridas
acc = accuracy_score(y_test, y_pred)
prec_macro = precision_score(y_test, y_pred, average="macro", zero_division=0)
recall_macro = recall_score(y_test, y_pred, average="macro", zero_division=0)
f1_macro = f1_score(y_test, y_pred, average="macro", zero_division=0)

print("\n=== GaussianNB: Métricas (macro) ===")
print(f"Accuracy:  {acc:.4f}")
print(f"Precision: {prec_macro:.4f}")
print(f"Recall:    {recall_macro:.4f}")
print(f"F1-score:  {f1_macro:.4f}")

print("\n=== GaussianNB: Reporte por clase ===")
print(classification_report(y_test, y_pred, target_names=target_names, zero_division=0))

# -----------------------------
# 2.B) Matriz de confusión
# -----------------------------
cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(6, 5), dpi=120)
plt.imshow(cm, interpolation="nearest", cmap=plt.cm.viridis) 
plt.title("Matriz de confusión (GaussianNB)")
plt.xlabel("Predicción")
plt.ylabel("Real")
plt.xticks(np.arange(len(target_names)), target_names, rotation=45, ha="right")
plt.yticks(np.arange(len(target_names)), target_names)
# Añadir los conteos a la matriz
for i in range(cm.shape[0]):
    for j in range(cm.shape[1]):
        plt.text(j, i, cm[i, j], ha="center", va="center", fontsize=11, 
                 color="white" if cm[i, j] > cm.max()/2 else "black")
plt.tight_layout()
plt.show()

# -----------------------------
# 3) Gráfica de Regiones de Decisión (Usando PCA 2D)
# -----------------------------
# Dividir los datos REDUCIDOS POR PCA
X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(
    X_pca, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=y
)

# Entrenar un nuevo clasificador 2D (sin scaler, ya que X_pca está escalado)
gnb_2d = GaussianNB()
gnb_2d.fit(X_train_pca, y_train_pca) 

# Crear la Malla (meshgrid) para mapear las regiones de decisión
x_min, x_max = X_pca[:, 0].min() - 0.5, X_pca[:, 0].max() + 0.5
y_min, y_max = X_pca[:, 1].min() - 0.5, X_pca[:, 1].max() + 0.5
xx, yy = np.meshgrid(
    np.linspace(x_min, x_max, 400),
    np.linspace(y_min, y_max, 400)
)
Z = gnb_2d.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)

# Graficar
cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA']) # Colores para el fondo de la clasificación
plt.figure(figsize=(7, 6), dpi=120)

# Contorno de regiones de decisión
plt.contourf(xx, yy, Z, cmap=cmap_light, alpha=0.5)

# Puntos de datos de prueba con su clase REAL para distinguir datos
for label in np.unique(y_test_pca):
    mask = (y_test_pca == label)
    pts = X_test_pca[mask]
    plt.scatter(pts[:, 0], pts[:, 1], label=target_names[label], s=30)

plt.title("GaussianNB: Regiones de Decisión (PCA 2D)")
plt.xlabel("Componente Principal 1 (PC1)")
plt.ylabel("Componente Principal 2 (PC2)")
plt.legend()
plt.tight_layout()
plt.show()
